                                       
                                       
                                       
                                          **ARO Lab 1**


This **Lab** is intended to give you a hands on introduction to using OpenShift from the perspective of a developer.

* Topics which this workshop will cover include:

* Using the OpenShift command line client and web console.

* Deploying an application using a pre-existing container image.

* Working with application labels to identify component parts.

* Scaling up your application in order to handle web traffic.

* Exposing your application to users outside of the cluster.

* Viewing and working with logs generated by your application.

* Accessing your application container and interacting with it.

* Architecture Overview of the ParksMap Application

This lab introduces you to the architecture of the ParksMap application used throughout this workshop, to get a better understanding of the things you’ll be doing from a developer perspective. ParksMap is a polyglot geo-spatial data visualization application built using the microservices architecture and is composed of a set of services which are developed using different programming languages and frameworks.

![image](https://user-images.githubusercontent.com/32516987/216835216-21049e46-6b7c-4b81-92ec-7508da7ef9bc.png)


The main service is a web application which has a server-side component in charge of aggregating the geo-spatial APIs provided by multiple independent backend services and a client-side component in JavaScript that is responsible for visualizing the geo-spatial data on the map. The client-side component which runs in your browser communicates with the server-side via WebSockets protocol in order to update the map in real-time.

There will be a set of independent backend services deployed that will provide different mapping and geo-spatial information. The set of available backend services that provide geo-spatial information are:

WorldWide National Parks

Major League Baseball Stadiums in North America

The original source code for this application is located here.

The server-side component of the ParksMap web application acts as a communication gateway to all the available backends. These backends will be dynamically discovered by using service discovery mechanisms provided by OpenShift which will be discussed in more details in the following labs.

In this lab, we’re going to deploy the web component of the ParksMap application which is also called parksmap and uses OpenShift’s service discovery mechanism to discover the backend services deployed and shows their data on the map.

![image](https://user-images.githubusercontent.com/32516987/216835267-1c33e8c5-80b3-4ba5-8a19-6bacd298b2ae.png)


Exercise: Deploying your First Image

Let’s start by doing the simplest thing possible - get a plain old Docker-formatted image to run on Azure Red Hat OpenShift. This is incredibly simple to do. With OpenShift it can be done directly from the web console.

If you’re no longer on the Topology view in the Developer perspective, return there now. Click Container Image to open a dialog that will allow you to specify the information for the image you want to deploy.

![image](https://user-images.githubusercontent.com/32516987/216835273-0c62a5b1-f1b1-44ad-a633-64f700057e78.png)




In the Image Name field, copy and paste the following into the box :

> quay.io/openshiftroadshow/parksmap:latest

OpenShift will then go out to the container registry specified and interrogate the image.

Your screen will end up looking something like this:

![image](https://user-images.githubusercontent.com/32516987/216835292-6fb2f896-9de1-4693-b1b6-15cde2152a37.png)




In Runtime Icon you can select the icon to use in OpenShift Topology View for the app. You can leave the default OpenShift icon, or select one from the list.

Make sure to have the correct values in:

* Application Name : workshop

* Name : parksmap

Ensure **Deployment** is selected from Resource section.

Un-check the checkbox next to **Create a route to the application.** For learning purposes, we will create a Route for the application later in the workshop.

At the bottom of the page, click Labels in the Advanced Options section and add some labels to better identify this deployment later. Labels will help us identify and filter components in the web console and in the command line.

We will add 3 labels. After you enter the name=value pair for each label, press tab or de-focus with mouse before typing the next. First the name to be given to the application.

> app=workshop

Next we apply the name of this deployment

> component=parksmap

And last, the role this component plays as part of the overall app

> role=frontend

![image](https://user-images.githubusercontent.com/32516987/216835436-35923116-8c34-48b0-8411-970f78210ec5.png)


Next, click the blue Create button. You will be directed to the Topology page, where you should see the visualization for the parksmap deployment config in the workshop application.

![image](https://user-images.githubusercontent.com/32516987/216835452-420083f4-d1dd-417e-a33e-d746be48983b.png)




These few steps are the only ones you need to run to get a container image deployed on OpenShift. This should work with any container image that follows best practices, such as defining an EXPOSE port, not needing to run specifically as the root user or other user name, and a single non-exiting CMD to execute on start.

Background: Containers and Pods

Before we start digging in, we need to understand how containers and Pods are related. We will not be covering the background on these technologies in this lab but if you have questions please inform the instructor. Instead, we will dive right in and start using them.

In OpenShift, the smallest deployable unit is a Pod. A Pod is a group of one or more OCI containers deployed together and guaranteed to be on the same host. From the official OpenShift documentation:

” Each Pod has its own IP address, therefore owning its entire port space, and containers within pods can share storage. Pods can be “tagged” with one or more labels, which are then used to select and manage groups of pods in a single operation. Pods can contain multiple OCI containers. The general idea is for a Pod to contain a “main process” and any auxiliary services you want to run along with that process. Examples of containers you might put in a Pod are, an Apache HTTPD server, a log analyzer, and a file service to help manage uploaded files. “

Exercise: Examining the Pod

If you click on the parksmap entry in the Topology view, you will see some information about that deployment config. The Resources tab may be displayed by default. If so, click on the Details tab. On that panel, you will see that there is a single Pod that was created by your actions.

![image](https://user-images.githubusercontent.com/32516987/216835468-6dd833ca-39e4-4f4f-a265-ca0da771daea.png)


You can also get a list of all the Pods created within your Project, by navigating to Workloads → Pods in the Administrator perspective of the web console.

![image](https://user-images.githubusercontent.com/32516987/216835474-20880643-07c6-4fc4-b69e-8bf16fdee169.png)



This Pod contains a single container, which happens to be the parksmap application - a simple Spring Boot/Java application.

You can also examine Pods from the command line:

> oc get pods

You should see output that looks similar to:

![image](https://user-images.githubusercontent.com/32516987/216835485-0061f5b0-71ba-4c89-96ba-b32b8d04c52f.png)


The above output lists all of the Pods in the current Project, including the Pod name, state, restarts, and uptime. Once you have a Pod’s name, you can get more information about the Pod using the oc get command. To make the output readable, I suggest changing the output type to YAML using the following syntax:

> oc get pod parksmap-65c4f8b676-k5gkk -o yaml

You should see something like the following output (which has been truncated due to space considerations of this workshop manual):

![image](https://user-images.githubusercontent.com/32516987/216835493-90d03fea-87bf-4471-a24e-3a14ef454f38.png)


The web interface also shows a lot of the same information on the Pod details page. If you click on the name of the Pod, you will find the details page. You can also get there by clicking on the parksmap deployment config on the Topology page, selecting Resources, and then clicking the Pod name.


![image](https://user-images.githubusercontent.com/32516987/216835501-8f505f1d-2fff-43e4-b9ea-adcac398f42a.png)

![image](https://user-images.githubusercontent.com/32516987/216835508-80b0c037-b9c3-45f8-946a-d7a03a585aaf.png)


Getting the parksmap image running may take a little while to complete. Each OpenShift node that is asked to run the image has to pull (download) it, if the node does not already have it cached locally. You can check on the status of the image download and deployment in the Pod details page, or from the command line with the oc get pods command that you used before.

**Background: Services**

Services provide a convenient abstraction layer inside OpenShift to find a group of similar Pods. They also act as an internal proxy/load balancer between those Pods and anything else that needs to access them from inside the OpenShift environment. For example, if you needed more parksmap instances to handle the load, you could spin up more Pods. OpenShift automatically maps them as endpoints to the Service, and the incoming requests would not notice anything different except that the Service was now doing a better job handling the requests.

When you asked OpenShift to run the image, it automatically created a Service for you. Remember that services are an internal construct. They are not available to the “outside world”, or anything that is outside the OpenShift environment. That’s okay, as you will learn later.

The way that a Service maps to a set of Pods is via a system of Labels and Selectors. Services are assigned a fixed IP address and many ports and protocols can be mapped.

There is a lot more information about Services, including the YAML format to make one by hand, in the official documentation.

Now that we understand the basics of what a Service is, let’s take a look at the Service that was created for the image that we just deployed. In order to view the Services defined in your Project, enter in the following command:

> oc get services

You should see something like this :

![image](https://user-images.githubusercontent.com/32516987/216835528-427cadc0-154c-4313-9663-0adca93472df.png)


In the above output, we can see that we have a Service named parksmap with an IP/Port combination of 172.30.22.209/8080TCP. Your IP address may be different, as each Service receives a unique IP address upon creation. Service IPs are fixed and never change for the life of the Service.

In the Developer perspective from the Topology view, service information is available by clicking the parksmap deployment config, then Resources, and then you should see the parksmap entry in the Services section.

![image](https://user-images.githubusercontent.com/32516987/216835536-f0642403-a04f-49ab-8097-9fbf35b32e2b.png)


You can also get more detailed information about a Service by using the following command to display the data in YAML:

> oc get service parksmap -o yaml

Which should provide an output like this :

![image](https://user-images.githubusercontent.com/32516987/216835547-eef875a7-2bcf-4611-93d0-168290ce0a88.png)



Take note of the selector stanza. Remember it.

Alternatively, you can use the web console to view information about the Service by clicking on it from the previous screen.

![image](https://user-images.githubusercontent.com/32516987/216835569-3d198030-489d-44c1-9179-867df60b6c8c.png)


It is also of interest to view the YAML of the Pod to understand how OpenShift wires components together. For example, run the following command to get the name of your parksmap Pod:

> oc get pods

You should get something like this :

![image](https://user-images.githubusercontent.com/32516987/216835574-b8d886b7-f124-4d1a-9eb7-f68f6ddc5ea7.png)


Now you can view the detailed data for your Pod with the following command:

> oc get pod parksmap-65c4f8b676-k5gkk -o yaml

Under the metadata section you should see the following:

![image](https://user-images.githubusercontent.com/32516987/216835590-ad036a98-6788-46e9-a07c-b8b350446cf4.png)


The Service has selector stanza that refers to deploymentconfig=parksmap.

The Pod has multiple Labels:

> app=parksmap

> deploymentconfig=parksmap

Labels are just key/value pairs. Any Pod in this Project that has a Label that matches the Selector will be associated with the Service. To see this in action, issue the following command:

> oc describe service parksmap

![image](https://user-images.githubusercontent.com/32516987/216835603-102e1d6c-0894-45c6-b0ec-06b57cbb33c8.png)



Now that you have succesfully deployed an application on ARO and you have seen how you can observe and disect it , you are ready to go to the 2nd part of the lab where we can see how an application running on ARO can be exposed to the public and get scaled up and down depending on demand and “revive” in case of a failed pod.
