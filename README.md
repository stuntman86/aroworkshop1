
 # For every resource that you deploy on ARO (project/app etc) please append your student name number to it. For example if the documentation mentions to create a parksmap app , student 1 should create parksmap1 , student 2 parksmap2 etc                                        
                                       
                                         
# Lab 1 : Deploy a simple application on ARO                                           
                                   

This **Lab** is intended to give you a hands on introduction to using OpenShift from the perspective of a developer.

* Topics which this workshop will cover include:

* Using the OpenShift command line client and web console.

* Deploying an application using a pre-existing container image.

* Working with application labels to identify component parts.

* Scaling up your application in order to handle web traffic.

* Exposing your application to users outside of the cluster.

* Viewing and working with logs generated by your application.

* Accessing your application container and interacting with it.

* Architecture Overview of the ParksMap Application

This lab introduces you to the architecture of the ParksMap application used throughout this workshop, to get a better understanding of the things you’ll be doing from a developer perspective. ParksMap is a polyglot geo-spatial data visualization application built using the microservices architecture and is composed of a set of services which are developed using different programming languages and frameworks.

![image](https://user-images.githubusercontent.com/32516987/216835216-21049e46-6b7c-4b81-92ec-7508da7ef9bc.png)


The main service is a web application which has a server-side component in charge of aggregating the geo-spatial APIs provided by multiple independent backend services and a client-side component in JavaScript that is responsible for visualizing the geo-spatial data on the map. The client-side component which runs in your browser communicates with the server-side via WebSockets protocol in order to update the map in real-time.

There will be a set of independent backend services deployed that will provide different mapping and geo-spatial information. The set of available backend services that provide geo-spatial information are:

WorldWide National Parks

Major League Baseball Stadiums in North America

The original source code for this application is located here.

The server-side component of the ParksMap web application acts as a communication gateway to all the available backends. These backends will be dynamically discovered by using service discovery mechanisms provided by OpenShift which will be discussed in more details in the following labs.

In this lab, we’re going to deploy the web component of the ParksMap application which is also called parksmap and uses OpenShift’s service discovery mechanism to discover the backend services deployed and shows their data on the map.

![image](https://user-images.githubusercontent.com/32516987/216835267-1c33e8c5-80b3-4ba5-8a19-6bacd298b2ae.png)


Exercise: Deploying your First Image

Let’s start by doing the simplest thing possible - get a plain old Docker-formatted image to run on Azure Red Hat OpenShift. This is incredibly simple to do. With OpenShift it can be done directly from the web console.

If you’re no longer on the Topology view in the Developer perspective, return there now. Click Container Image to open a dialog that will allow you to specify the information for the image you want to deploy.

![image](https://user-images.githubusercontent.com/32516987/216835273-0c62a5b1-f1b1-44ad-a633-64f700057e78.png)




In the Image Name field, copy and paste the following into the box :

> quay.io/openshiftroadshow/parksmap:latest

OpenShift will then go out to the container registry specified and interrogate the image.

Your screen will end up looking something like this:

![image](https://user-images.githubusercontent.com/32516987/216835292-6fb2f896-9de1-4693-b1b6-15cde2152a37.png)




In Runtime Icon you can select the icon to use in OpenShift Topology View for the app. You can leave the default OpenShift icon, or select one from the list.

Make sure to have the correct values in:

* Application Name : workshop

* Name : parksmap

Ensure **Deployment** is selected from Resource section.

Un-check the checkbox next to **Create a route to the application.** For learning purposes, we will create a Route for the application later in the workshop.

At the bottom of the page, click Labels in the Advanced Options section and add some labels to better identify this deployment later. Labels will help us identify and filter components in the web console and in the command line.

We will add 3 labels. After you enter the name=value pair for each label, press tab or de-focus with mouse before typing the next. First the name to be given to the application.

> app=workshop

Next we apply the name of this deployment

> component=parksmap

And last, the role this component plays as part of the overall app

> role=frontend

![image](https://user-images.githubusercontent.com/32516987/216835436-35923116-8c34-48b0-8411-970f78210ec5.png)


Next, click the blue Create button. You will be directed to the Topology page, where you should see the visualization for the parksmap deployment config in the workshop application.

![image](https://user-images.githubusercontent.com/32516987/216835452-420083f4-d1dd-417e-a33e-d746be48983b.png)




These few steps are the only ones you need to run to get a container image deployed on OpenShift. This should work with any container image that follows best practices, such as defining an EXPOSE port, not needing to run specifically as the root user or other user name, and a single non-exiting CMD to execute on start.

Background: Containers and Pods

Before we start digging in, we need to understand how containers and Pods are related. We will not be covering the background on these technologies in this lab but if you have questions please inform the instructor. Instead, we will dive right in and start using them.

In OpenShift, the smallest deployable unit is a Pod. A Pod is a group of one or more OCI containers deployed together and guaranteed to be on the same host. From the official OpenShift documentation:

” Each Pod has its own IP address, therefore owning its entire port space, and containers within pods can share storage. Pods can be “tagged” with one or more labels, which are then used to select and manage groups of pods in a single operation. Pods can contain multiple OCI containers. The general idea is for a Pod to contain a “main process” and any auxiliary services you want to run along with that process. Examples of containers you might put in a Pod are, an Apache HTTPD server, a log analyzer, and a file service to help manage uploaded files. “

Exercise: Examining the Pod

If you click on the parksmap entry in the Topology view, you will see some information about that deployment config. The Resources tab may be displayed by default. If so, click on the Details tab. On that panel, you will see that there is a single Pod that was created by your actions.

![image](https://user-images.githubusercontent.com/32516987/216835468-6dd833ca-39e4-4f4f-a265-ca0da771daea.png)


You can also get a list of all the Pods created within your Project, by navigating to Workloads → Pods in the Administrator perspective of the web console.

![image](https://user-images.githubusercontent.com/32516987/216835474-20880643-07c6-4fc4-b69e-8bf16fdee169.png)



This Pod contains a single container, which happens to be the parksmap application - a simple Spring Boot/Java application.

You can also examine Pods from the command line:

> oc get pods

You should see output that looks similar to:

![image](https://user-images.githubusercontent.com/32516987/216835485-0061f5b0-71ba-4c89-96ba-b32b8d04c52f.png)


The above output lists all of the Pods in the current Project, including the Pod name, state, restarts, and uptime. Once you have a Pod’s name, you can get more information about the Pod using the oc get command. To make the output readable, I suggest changing the output type to YAML using the following syntax:

> oc get pod parksmap-65c4f8b676-k5gkk -o yaml

You should see something like the following output (which has been truncated due to space considerations of this workshop manual):

![image](https://user-images.githubusercontent.com/32516987/216835493-90d03fea-87bf-4471-a24e-3a14ef454f38.png)


The web interface also shows a lot of the same information on the Pod details page. If you click on the name of the Pod, you will find the details page. You can also get there by clicking on the parksmap deployment config on the Topology page, selecting Resources, and then clicking the Pod name.


![image](https://user-images.githubusercontent.com/32516987/216835501-8f505f1d-2fff-43e4-b9ea-adcac398f42a.png)

![image](https://user-images.githubusercontent.com/32516987/216835508-80b0c037-b9c3-45f8-946a-d7a03a585aaf.png)


Getting the parksmap image running may take a little while to complete. Each OpenShift node that is asked to run the image has to pull (download) it, if the node does not already have it cached locally. You can check on the status of the image download and deployment in the Pod details page, or from the command line with the oc get pods command that you used before.

**Background: Services**

Services provide a convenient abstraction layer inside OpenShift to find a group of similar Pods. They also act as an internal proxy/load balancer between those Pods and anything else that needs to access them from inside the OpenShift environment. For example, if you needed more parksmap instances to handle the load, you could spin up more Pods. OpenShift automatically maps them as endpoints to the Service, and the incoming requests would not notice anything different except that the Service was now doing a better job handling the requests.

When you asked OpenShift to run the image, it automatically created a Service for you. Remember that services are an internal construct. They are not available to the “outside world”, or anything that is outside the OpenShift environment. That’s okay, as you will learn later.

The way that a Service maps to a set of Pods is via a system of Labels and Selectors. Services are assigned a fixed IP address and many ports and protocols can be mapped.

There is a lot more information about Services, including the YAML format to make one by hand, in the official documentation.

Now that we understand the basics of what a Service is, let’s take a look at the Service that was created for the image that we just deployed. In order to view the Services defined in your Project, enter in the following command:

> oc get services

You should see something like this :

![image](https://user-images.githubusercontent.com/32516987/216835528-427cadc0-154c-4313-9663-0adca93472df.png)


In the above output, we can see that we have a Service named parksmap with an IP/Port combination of 172.30.22.209/8080TCP. Your IP address may be different, as each Service receives a unique IP address upon creation. Service IPs are fixed and never change for the life of the Service.

In the Developer perspective from the Topology view, service information is available by clicking the parksmap deployment config, then Resources, and then you should see the parksmap entry in the Services section.

![image](https://user-images.githubusercontent.com/32516987/216835536-f0642403-a04f-49ab-8097-9fbf35b32e2b.png)


You can also get more detailed information about a Service by using the following command to display the data in YAML:

> oc get service parksmap -o yaml

Which should provide an output like this :

![image](https://user-images.githubusercontent.com/32516987/216835547-eef875a7-2bcf-4611-93d0-168290ce0a88.png)



Take note of the selector stanza. Remember it.

Alternatively, you can use the web console to view information about the Service by clicking on it from the previous screen.

![image](https://user-images.githubusercontent.com/32516987/216835569-3d198030-489d-44c1-9179-867df60b6c8c.png)


It is also of interest to view the YAML of the Pod to understand how OpenShift wires components together. For example, run the following command to get the name of your parksmap Pod:

> oc get pods

You should get something like this :

![image](https://user-images.githubusercontent.com/32516987/216835574-b8d886b7-f124-4d1a-9eb7-f68f6ddc5ea7.png)


Now you can view the detailed data for your Pod with the following command:

> oc get pod parksmap-65c4f8b676-k5gkk -o yaml

Under the metadata section you should see the following:

![image](https://user-images.githubusercontent.com/32516987/216835590-ad036a98-6788-46e9-a07c-b8b350446cf4.png)


The Service has selector stanza that refers to deploymentconfig=parksmap.

The Pod has multiple Labels:

> app=parksmap

> deploymentconfig=parksmap

Labels are just key/value pairs. Any Pod in this Project that has a Label that matches the Selector will be associated with the Service. To see this in action, issue the following command:

> oc describe service parksmap

![image](https://user-images.githubusercontent.com/32516987/216835603-102e1d6c-0894-45c6-b0ec-06b57cbb33c8.png)



Now that you have succesfully deployed an application on ARO and you have seen how you can observe and disect it , you are ready to go to the 2nd part of the lab where we can see how an application running on ARO can get scaled up and down depending on demand and “revive” in case of a failed pod.

# Lab 2: Scaling up and down / Self Healing

**Background: Deployments and ReplicaSets** 

While **Services** provide routing and load balancing for **Pods**, which may go in and out of existence, **ReplicaSet (RS)** and **ReplicationController (RC)** are used to specify and then ensure the desired number of **Pods** (replicas) are in existence. For example, if you always want your application server to be scaled to **3 Pods (instances)**, a **ReplicaSet** is needed. Without an **RS**, any **Pods** that are killed or somehow die/exit are not automatically restarted. **ReplicaSets** and **ReplicationController** are how **Azure Red Hat OpenShift** "self heals" and while **Deployments** control **ReplicaSets**, **ReplicationController** here are controlled by **DeploymentConfigs**.

In Kubernetes, a Deployment (D) defines how something should be deployed. In almost all cases, you will end up using the Pod, Service, ReplicaSet and Deployment resources together. And, in almost all of those cases, OpenShift will create all of them for you.

There are some edge cases where you might want some Pods and an RS without a D or a Service, and others, so feel free to ask us about them after the labs.


## Exercise: Exploring Deployment-related Objects

Now that we know the background of what a ReplicaSet and Deployment are, we can explore how they work and are related. Take a look at the Deployment (D) that was created for you when you told OpenShift to stand up the parksmap image:

> oc get deployment

![image](https://user-images.githubusercontent.com/32516987/216836256-d51041d6-2a1d-45af-8c8a-a2ec754a984e.png)

To get more details, we can look into the ReplicaSet (RS).

Take a look at the ReplicaSet (RS) that was created for you when you told OpenShift to stand up the parksmap image:

> oc get rs

![image](https://user-images.githubusercontent.com/32516987/216836292-0b22e3fe-7540-4e41-83c2-37eb01c033af.png)

This lets us know that, right now, we expect one Pod to be deployed (Desired), and we have one Pod actually deployed (Current). By changing the desired number, we can tell OpenShift that we want more or less Pods.

OpenShift’s HorizontalPodAutoscaler effectively monitors the CPU usage of a set of instances and then manipulates the RCs accordingly.

## Exercise: Scaling the Application

Let’s scale our parksmap "application" up to 2 instances. We can do this with the scale command. You could also do this by incrementing the Desired Count in the OpenShift web console. Pick one of these methods; it’s your choice.

> oc scale --replicas=2 deployment/parksmap

You can also scale up to two pods in the Developer Perspective. From the Topology view, first click the parksmap deployment config and select the Details tab:

![image](https://user-images.githubusercontent.com/32516987/216836353-ef48a60f-d0a7-478f-bbb9-82f164d3a2d1.png)

Next, click the ^ icon next to the Pod visualization to scale up to 2 pods.

![image](https://user-images.githubusercontent.com/32516987/216836367-3edac46e-3e32-4765-805f-da9a64c93c1a.png)

To verify that we changed the number of replicas, issue the following command:

> oc get rs

![image](https://user-images.githubusercontent.com/32516987/216836397-3d0656b3-bfe8-480c-b7b7-d8d4494f49e7.png)

You can see that we now have 2 replicas. Let’s verify the number of pods with the oc get pods command:

> oc get pods


![image](https://user-images.githubusercontent.com/32516987/216836419-149a50a1-dc9b-428f-aed3-03e91b9dd8d4.png)

And lastly, let’s verify that the Service that we learned about in the previous lab accurately reflects two endpoints:

> oc describe svc parksmap

You will get soemthing similar to this:

![image](https://user-images.githubusercontent.com/32516987/216836538-4abc5524-c201-43cc-aaf9-f6a2a9507713.png)

Another way to look at a Service's endpoints is with the following:

> oc get endpoints parksmap

which will result in something like this :

![image](https://user-images.githubusercontent.com/32516987/216836575-353eb823-0ff2-4ec5-9da0-d59d6afdd258.png)

Your IP addresses will likely be different, as each pod receives a unique IP within the OpenShift environment. The endpoint list is a quick way to see how many pods are behind a service.

You can also see that both Pods are running in the Developer Perspective:      

![image](https://user-images.githubusercontent.com/32516987/216836600-76e1e45d-e24e-4212-82e9-4bdbdb5d788d.png)

Overall, that’s how simple it is to scale an application (Pods in a Service). Application scaling can happen extremely quickly because OpenShift is just launching new instances of an existing image, especially if that image is already cached on the node.

## Application "Self Healing"

Because OpenShift’s RSs are constantly monitoring to see that the desired number of Pods actually are running, you might also expect that OpenShift will "fix" the situation if it is ever not right. You would be correct!

Since we have two Pods running right now, let’s see what happens if we "accidentally" kill one. Run the oc get pods command again, and choose a Pod name. Then, do the following:

> oc delete pod parksmap-65c4f8b676-k5gkk && oc get pods

![image](https://user-images.githubusercontent.com/32516987/216836636-f703d0ad-3440-4013-b7a6-96eff6a2b5d0.png)


Did you notice anything? One container has been deleted, and there’s a new container already being created.

Also, the names of the Pods are slightly changed. That’s because OpenShift almost immediately detected that the current state (1 Pod) didn’t match the desired state (2 Pods), and it fixed it by scheduling another Pod.

Additionally, OpenShift provides rudimentary capabilities around checking the liveness and/or readiness of application instances. If the basic checks are insufficient, OpenShift also allows you to run a command inside the container in order to perform the check. That command could be a complicated script that uses any installed language.

Based on these health checks, if OpenShift decided that our parksmap application instance wasn’t alive, it would kill the instance and then restart it, always ensuring that the desired number of replicas was in place.

## Exercise : Scale Down

Before we continue, go ahead and scale your application down to a single instance. Feel free to do this using whatever method you like.


# Exposing your Application to the Outside World


In this lab, we’re going to make our application visible to the end users, so they can access it.

![image](https://user-images.githubusercontent.com/32516987/216836689-1b5f6169-9d32-40e1-966f-1a71f468c70d.png)


## Routes

While Services provide internal abstraction and load balancing within an OpenShift environment, sometimes clients (users, systems, devices, etc.) outside of OpenShift need to access an application. The way that external clients are able to access applications running in OpenShift is through the OpenShift routing layer. And the data object behind that is a Route.

The default OpenShift router (HAProxy) uses the HTTP header of the incoming request to determine where to proxy the connection. You can optionally define security, such as TLS, for the Route. If you want your Services, and, by extension, your Pods, to be accessible from the outside world, you need to create a Route.

## Exercise : Create a Route

You may remember that when we deployed the parksmap application, we un-checked the checkbox to create a Route. Normally it would have been created for us automatically. Fortunately, creating a Route is a pretty straight-forward process. You simply expose the Service via the command line. Or, via the Administrator Perspective, just click Networking → Routes and then the Create Route button.

Insert parksmap in Name field.

From Service field, select parksmap. For Target Port, select 8080.

In Security section, check Secure route. Select Edge from TLS Termination list.

Leave all other fields blank and click Create:


![image](https://user-images.githubusercontent.com/32516987/216836732-8063ded2-c8e3-4116-9edd-b28ff0c190f5.png)


![image](https://user-images.githubusercontent.com/32516987/216836738-cb5e3711-8a76-411b-85be-808810b55a23.png)

When creating a Route, some other options can be provided, like the hostname and path for the Route or the other TLS configurations.

When using the command line, we can first verify that we don’t already have any existing Routes:

> oc get routes

> No resources found

Now lets get the service name we want to expose

> oc get services

![image](https://user-images.githubusercontent.com/32516987/216836767-677ac081-a6a3-458e-9b2d-8756a9ffb23a.png)


Once we know the Service name, creating a Route is a simple one-command task:


> oc create route edge parksmap --service=parksmap

> route.route.openshift.io/parksmap exposed

Verify the Route was created with the following command:

> oc get route

![image](https://user-images.githubusercontent.com/32516987/216836795-1876f013-551c-4414-975e-693ece390ca2.png)


You can also verify the Route in the Developer Perspective under the Resources tab for your parksmap deployment configuration. Also note that there is a decorator icon on the parksmap visualization now. If you click that, it will open the URL for your Route in a browser.

![image](https://user-images.githubusercontent.com/32516987/216836805-7392f2e9-0b61-4158-8c20-3dbde8d858ce.png)


This application is now available at the URL shown in the Developer Perspective. Click the link and you will see it.

> At first time, the Browser will ask permission to get your position. This is needed by the Frontend app to center the world map to your location, if you don’t allow it, it will just use a default location.

![image](https://user-images.githubusercontent.com/32516987/216836825-85748a63-0136-450c-86f0-944bd683b3df.png)


# Exploring ARO's Logging Capabilities

OpenShift provides some convenient mechanisms for viewing application logs. First and foremost is the ability to examine a Pod's logs directly from the web console or via the command line.

## Exercise: Examining logs

Since we already deployed our application, we can take some time to examine its logs. In the Developer Perspective, from Topology view, click the parksmap entry and then the Resources tab. You should see a View Logs link next to the Pod entry.

![image](https://user-images.githubusercontent.com/32516987/216836886-d6a2a8b2-b26e-41da-a908-92d6bb24b69b.png)


Click the View Logs link and you should see a nice view of the Pod's logs:

![image](https://user-images.githubusercontent.com/32516987/216836892-0a2dd1ac-3f76-4d11-87d8-361cc48b1da0.png)


You also have the option of viewing logs from the command line. Get the name of your Pod:

> oc get pods

![image](https://user-images.githubusercontent.com/32516987/216836915-fe7c7374-8362-4096-a470-3b5ebe2ccbd8.png)

And then use the logs command to view this Pod's logs:

> oc logs parksmap-1-hx0kv


You will see all of the application logs scroll on your screen:

![image](https://user-images.githubusercontent.com/32516987/216836941-73f73302-8c3f-4a1e-a1fc-6462e0a2acf6.png)


#Lab 2 : ARO Internals (if you would like to continue experimenting on different stuff of your choice on ARO instead , please go ahead)

We will use an application named OStoy that will help us with this Lab.

### About OSToy
OSToy is a simple Node.js application that we will deploy to Azure Red Hat OpenShift. It is used to help us explore the functionality of Kubernetes. This application has a user interface which you can:

* write messages to the log (stdout / stderr)
* intentionally crash the application to view self-healing
* toggle a liveness probe and monitor OpenShift behavior
* read config maps, secrets, and env variables
* if connected to shared storage, read and write files
* check network connectivity, intra-cluster DNS, and intra-communication with an included microservice
* increase the load to view automatic scaling of the pods to handle the load (via the Horizontal Pod Autoscaler)

### OSToy Application Diagram

![image](https://user-images.githubusercontent.com/32516987/216837205-69efefdb-c3e7-448f-9a48-4cc3f5392929.png)


### Familiarization with the Application UI

1. Shows the pod name that served your browser the page.
2. Home: The main page of the application where you can perform some of the functions listed which we will explore.
3. Persistent Storage: Allows us to write data to the persistent volume bound to this application.
4. Config Maps: Shows the contents of configmaps available to the application and the key:value pairs.
5. Secrets: Shows the contents of secrets available to the application and the key:value pairs.
6. ENV Variables: Shows the environment variables available to the application.
7. Auto Scaling: Explore the Horizontal Pod Autoscaler to see how increased loads are handled.
8. Networking: Tools to illustrate networking within the application.
9. About: Shows some more information about the application.

![image](https://user-images.githubusercontent.com/32516987/216837264-2d5d56b4-7522-4d2b-9a79-78f5a2144ef5.png)


### Learn more about the application

To learn more, click on the “About” menu item on the left once we deploy the app.

![image](https://user-images.githubusercontent.com/32516987/216837290-86149fdd-95b9-4efa-aec8-bbf5cd8baf93.png)

## Application Deployment

### Create new project

Create a new project called "ostoy" in your cluster

> oc new-project ostoy

You can do the same from the Web UI if you prefer : 

![image](https://user-images.githubusercontent.com/32516987/216837381-c63e2e07-4f9d-449c-8db4-8624e0fb1ce6.png)


### Deploy the backend microservice

In your terminal deploy the microservice using the following command:

> oc apply -f https://raw.githubusercontent.com/microsoft/aroworkshop/master/yaml/ostoy-microservice-deployment.yaml

You should get the following response: 



> oc apply -f https://raw.githubusercontent.com/microsoft/aroworkshop/master/yaml/ostoy-microservice-deployment.yaml
deployment.apps/ostoy-microservice created
service/ostoy-microservice-svc created



### Deploy the frontent service

This deployment contains the node.js frontend for our application along with a few other Kubernetes objects.

> oc apply -f https://raw.githubusercontent.com/microsoft/aroworkshop/master/yaml/ostoy-fe-deployment.yaml

You should see all objects created successfully

> oc apply -f https://raw.githubusercontent.com/microsoft/aroworkshop/master/yaml/ostoy-fe-deployment.yaml
persistentvolumeclaim/ostoy-pvc created
deployment.apps/ostoy-frontend created
service/ostoy-frontend-svc created
route.route.openshift.io/ostoy-route created
configmap/ostoy-configmap-env created
secret/ostoy-secret-env created
configmap/ostoy-configmap-files created
secret/ostoy-secret created

### Get Route

Get the route so that we can access the application via:



> oc get route

You should get something like this: 

![image](https://user-images.githubusercontent.com/32516987/216837655-07da2dbd-d15a-484b-a557-1a6a48568d43.png)

Copy the generated route and copy paste it on your browser. You will see the homepage of the app.

![image](https://user-images.githubusercontent.com/32516987/216837671-2d43f3ac-1239-40e9-9187-669a244fee78.png)


### Logging and Metrics

Assuming you can access the application via the Route provided and are still logged into the CLI (please go back to part 2 if you need to do any of those) we’ll start to use this application. As stated earlier, this application will allow you to “push the buttons” of OpenShift and see how it works. We will do this to test the logs.

Click on the Home menu item and then click in the message box for “Log Message (stdout)” and write any message you want to output to the stdout stream. You can try “All is well!”. Then click “Send Message”.

![image](https://user-images.githubusercontent.com/32516987/216837696-c72d53d6-75a8-452d-aa6f-fb9d813f939d.png)

Click in the message box for “Log Message (stderr)” and write any message you want to output to the stderr stream. You can try “Oh no! Error!”. Then click “Send Message”.

![image](https://user-images.githubusercontent.com/32516987/216837701-9d0c8541-14c9-465d-847b-b4f814b4bc34.png)


### View Logs directly from the pod 

Go to the CLI and enter the following command to retrieve the name of your frontend pod which we will use to view the pod logs:

> oc get pods -o name
pod/ostoy-frontend-679cb85695-5cn7x
pod/ostoy-microservice-86b4c6f559-p594d

So the pod name in this case is ostoy-frontend-679cb85695-5cn7x. Then run oc logs ostoy-frontend-679cb85695-5cn7x and you should see your messages:

> oc logs ostoy-frontend-679cb85695-5cn7x
[...]
ostoy-frontend-679cb85695-5cn7x: server starting on port 8080
Redirecting to /home
stdout: All is well!
stderr: Oh no! Error!

You should see both the stdout and stderr messages.

Try to see them from within the OpenShift Web Console as well. Make sure you are in the “ostoy” project. In the left menu click Workloads > Pods > <frontend-pod-name>. Then click the “Logs” sub-tab.
 
 
![image](https://user-images.githubusercontent.com/32516987/216837749-38a5929b-32f9-4fca-a2fd-d6a1410a0d9b.png)

### Exploring Health Checks
 
 In this section we will intentionally crash our pods and also make a pod non-responsive to the liveness probes and see how Kubernetes behaves. We will first intentionally crash our pod and see that Kubernetes will self-heal by immediately spinning it back up. Then we will trigger the health check by stopping the response on the /health endpoint in our app. After three consecutive failures, Kubernetes should kill the pod and then recreate it.

 
 It would be best to prepare by splitting your screen between the OpenShift Web Console and the OSToy application so that you can see the results of our actions immediately.

![image](https://user-images.githubusercontent.com/32516987/216837795-f7cd0576-c6cd-4a71-8014-350095047080.png)

 
 But if your screen is too small or that just won’t work, then open the OSToy application in another tab so you can quickly switch to the OpenShift Web Console once you click the button. To get to this deployment in the OpenShift Web Console go to the left menu and click:

Workloads > Deployments > “ostoy-frontend”

Go to the browser tab that has your OSToy app, click on Home in the left menu, and enter a message in the “Crash Pod” tile (e.g., “This is goodbye!”) and press the “Crash Pod” button. This will cause the pod to crash and Kubernetes should restart the pod. After you press the button you will see:
 
 ![image](https://user-images.githubusercontent.com/32516987/216837818-b58beb34-e57d-4a17-a676-f9af1e40f042.png)

 
 Quickly switch to the tab with the deployment showing in the web console. You will see that the pod turns yellowish, meaning it is down but should quickly come back up and show blue. It does happen quickly so you might miss it.
 
 ![image](https://user-images.githubusercontent.com/32516987/216837828-4da977e1-cd37-411b-ab85-749df4952907.png)

 
 You can also check in the pod events and further verify that the container has crashed and been restarted.

Click on Pods > [Pod Name] > Events
 
 ![image](https://user-images.githubusercontent.com/32516987/216837870-e6a5b9f5-eb4d-4170-950c-6925ee9ad562.png)
 
 
 Keep the page from the pod events still open from the previous step. Then in the OSToy app click on the “Toggle Health” button, in the “Toggle health status” tile. You will see the “Current Health” switch to “I’m not feeling all that well”.

![image](https://user-images.githubusercontent.com/32516987/216837879-511bc64f-8f40-4221-90eb-00db430f8f15.png)

This will cause the app to stop responding with a “200 HTTP code”. After 3 such consecutive failures (“A”), Kubernetes will kill the pod (“B”) and restart it (“C”). Quickly switch back to the pod events tab and you will see that the liveness probe failed and the pod as being restarted.
 
 ![image](https://user-images.githubusercontent.com/32516987/216837885-4050aa20-ff15-412f-87a7-80092fd7234d.png)
 
 ### Persistent Storage

 In this section we will execute a simple example of using persistent storage by creating a file that will be stored on a persistent volume in our cluster and then confirm that it will “persist” across pod failures and recreation.

nside the OpenShift web console click on Storage > Persistent Volume Claims in the left menu. You will then see a list of all persistent volume claims that our application has made. In this case there is just one called “ostoy-pvc”. If you click on it you will also see other pertinent information such as whether it is bound or not, size, access mode and creation time.

In this case the mode is RWO (Read-Write-Once) which means that the volume can only be mounted to one node, but the pod(s) can both read and write to that volume. The default in ARO is for Persistent Volumes to be backed by Azure Disk, but it is possible to use Azure Files so that you can use the RWX (Read-Write-Many) access mode. See here for more info on access modes.

In the OSToy app click on Persistent Storage in the left menu. In the “Filename” area enter a filename for the file you will create (e.g., “test-pv.txt”). Use the “.txt” extension so you can easily open it in the browser.

Underneath that, in the “File contents” box, enter text to be stored in the file. (e.g., “Azure Red Hat OpenShift is the greatest thing since sliced bread!”). Then click “Create file”.
 
 ![image](https://user-images.githubusercontent.com/32516987/216837922-f5a3179a-5cbc-43d8-b167-42aeddb48a0f.png)
 
 You will then see the file you created appear above under “Existing files”. Click on the file and you will see the filename and the contents you entered.

![image](https://user-images.githubusercontent.com/32516987/216837929-a2ae806d-da70-4f26-9699-217a46b31256.png)
 
 We now want to kill the pod and ensure that the new pod that spins up will be able to see the file we created. Exactly like we did in the previous section. Click on Home in the left menu.

Click on the “Crash pod” button. (You can enter a message if you’d like).

Click on Persistent Storage in the left menu.

You will see the file you created is still there and you can open it to view its contents to confirm.

 ![image](https://user-images.githubusercontent.com/32516987/216837937-71637294-d4fa-41aa-b232-9635c0835916.png)

Now let’s confirm that it’s actually there by using the CLI and checking if it is available to the container. If you remember we mounted the directory /var/demo_files to our PVC. So get the name of your frontend pod:
 
> oc get pods
 
 then get an SSH session into the container 
 
 > oc rsh <pod name>
 
 then > cd /var/demo_files
 
 if you enter **ls** you can see all the files you created. Next, let’s open the file we created and see the contents
 
 
> cat test-pv.txt
 
 You should see the text you entered in the UI.

 ![image](https://user-images.githubusercontent.com/32516987/216838064-24ac8507-239b-4b1c-9d3b-1253579ec55d.png)


Then exit the SSH session by typing exit. You will then be in your CLI.
 
 
 ## Configuration
 
 In this section we’ll take a look at how OSToy can be configured using ConfigMaps, Secrets, and Environment Variables. This section won’t go into details explaining each , but will show you how they are exposed to the application.
 
 

### Configuration using ConfigMaps

 Click on Config Maps in the left menu.

This will display the contents of the configmap available to the OSToy application. We defined this in the ostoy-fe-deployment.yaml here:

> kind: ConfigMap
apiVersion: v1
metadata:
  name: ostoy-configmap-files
data:
  config.json:  '{ "default": "123" }'
 
 ### Configuration with Secrets
 
 Kubernetes Secret objects allow you to store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys. Putting this information in a secret is safer and more flexible than putting it, verbatim, into a pod definition or a container image.

Click on Secrets in the left menu.

This will display the contents of the secrets available to the OSToy application. We defined this in the ostoy-fe-deployment.yaml here:
 
 > apiVersion: v1
kind: Secret
metadata:
  name: ostoy-secret
data:
  secret.txt: VVNFUk5BTUU9bXlfdXNlcgpQQVNTV09SRD1AT3RCbCVYQXAhIzYzMlk1RndDQE1UUWsKU01UUD1sb2NhbGhvc3QKU01UUF9QT1JUPTI1
type: Opaque
 
 ### Configuration using env variables
 
Using environment variables is an easy way to change application behavior without requiring code changes. It allows different deployments of the same application to potentially behave differently based on the environment variables, and OpenShift makes it simple to set, view, and update environment variables for Pods/Deployments.
 
 Click on ENV Variables in the left menu.

This will display the environment variables available to the OSToy application. We added three as defined in the deployment spec of ostoy-fe-deployment.yaml here:
 
 >  env:
  - name: ENV_TOY_CONFIGMAP
    valueFrom:
      configMapKeyRef:
        name: ostoy-configmap-env
        key: ENV_TOY_CONFIGMAP
  - name: ENV_TOY_SECRET
    valueFrom:
      secretKeyRef:
        name: ostoy-secret-env
        key: ENV_TOY_SECRET
  - name: MICROSERVICE_NAME
    value: OSTOY_MICROSERVICE_SVC
 
 
 The last one, **MICROSERVICE_NAME** is used for the intra-cluster communications between pods for this application. The application looks for this environment variable to know how to access the microservice in order to get the colors.
 

## Networking and Scaling
 In this section we’ll see how OSToy uses intra-cluster networking to separate functions by using microservices and visualize the scaling of pods.

Let’s review how this application is set up…
 
 ![image](https://user-images.githubusercontent.com/32516987/216838994-cd3ad84b-1025-4a64-8111-4b260c91426f.png)

 
 As can be seen in the image above we have defined at least 2 separate pods, each with its own service. One is the frontend web application (with a service and a publicly accessible route) and the other is the backend microservice with a service object created so that the frontend pod can communicate with the microservice (across the pods if more than one). Therefore this microservice is not accessible from outside this cluster (or from other namespaces/projects, if configured, due to OpenShifts’ network policy, ovs-networkpolicy). The sole purpose of this microservice is to serve internal web requests and return a JSON object containing the current hostname and a randomly generated color string. This color string is used to display a box with that color displayed in the tile titled “Intra-cluster Communication”.
 
 ### Networking
 
 Click on Networking in the left menu. Review the networking configuration.

The right tile titled “Hostname Lookup” illustrates how the service name created for a pod can be used to translate into an internal ClusterIP address. Enter the name of the microservice following the format of my-svc.my-namespace.svc.cluster.local which we created in our ostoy-microservice.yaml as seen here:
 
 > apiVersion: v1
kind: Service
metadata:
  name: ostoy-microservice-svc
  labels:
    app: ostoy-microservice
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
  selector:
    app: ostoy-microservice
 
 In this case we will enter: 
 
 > ostoy-microservice-svc.ostoy.svc.cluster.local
 
 We will see an IP address returned. In our example it is 172.30.165.246. This is the intra-cluster IP address; only accessible from within the cluster.

![image](https://user-images.githubusercontent.com/32516987/216839034-cd738f14-5ff1-4079-abd9-2aabf089c2cd.png)

 
 ## Scaling
 
 OpenShift allows one to scale up/down the number of pods for each part of an application as needed. This can be accomplished via changing our replicaset/deployment definition (declarative), by the command line (imperative), or via the web console (imperative). In our deployment definition (part of our ostoy-fe-deployment.yaml) we stated that we only want one pod for our microservice to start with. This means that the Kubernetes Replication Controller will always strive to keep one pod alive. We can also define pod autoscaling using the Horizontal Pod Autoscaler (HPA) based on load to expand past what we defined. We will do this in a later section of this lab.

If we look at the tile on the left we should see one box randomly changing colors. This box displays the randomly generated color sent to the frontend by our microservice along with the pod name that sent it. Since we see only one box that means there is only one microservice pod. We will now scale up our microservice pods and will see the number of boxes change.

To confirm that we only have one pod running for our microservice, run the following command, or use the web console.
 
 ![image](https://user-images.githubusercontent.com/32516987/216839077-fa173eac-29bf-409d-ae0c-559bee46652d.png)

 
 Let’s change our microservice definition yaml to reflect that we want 3 pods instead of the one we see. Download the ostoy-microservice-deployment.yaml and save it on your local machine.

Open the file using your favorite editor. Ex: vi ostoy-microservice-deployment.yaml.

Find the line that states replicas: 1 and change that to replicas: 3. Then save and quit.

It will look like this
 
 > spec:
    selector:
      matchLabels:
        app: ostoy-microservice
    replicas: 3
 
 Assuming you are still logged in via the CLI, execute the following command:
 
 > oc apply -f ostoy-microservice-deployment.yaml
 
 Confirm that there are now 3 pods via the CLI (oc get pods) or the web console (Workloads > Deployments > ostoy-microservice).

See this visually by visiting the OSToy app and seeing how many boxes you now see. It should be three.
 
  ![image](https://user-images.githubusercontent.com/32516987/216839095-293b6468-6b69-4c4f-b83e-bcce0d02b1dc.png)
 
 Now we will scale the pods down using the command line. Execute the following command from the CLI:
 
 > oc scale deployment ostoy-microservice --replicas=2

Confirm that there are indeed 2 pods, via the CLI (oc get pods) or the web console.

See this visually by visiting the OSToy App and seeing how many boxes you now see. It should be two.

Lastly, let’s use the web console to scale back down to one pod. Make sure you are in the project you created for this app (i.e., “ostoy”), in the left menu click Workloads > Deployments > ostoy-microservice. On the left you will see a blue circle with the number 2 in the middle. Click on the down arrow to the right of that to scale the number of pods down to 1.
 
 ![image](https://user-images.githubusercontent.com/32516987/216839113-69f85d33-74a7-4966-9919-6f3597b56d6e.png)



 See this visually by visiting the OSToy app and seeing how many boxes you now see. It should be one. You can also confirm this via the CLI or the web console.
 
 ### Pod AutoScaling
 
 In this section we will explore how the Horizontal Pod Autoscaler (HPA) can be used and works within Kubernetes/OpenShift.
 
 In more simple words, “if there is a lot of work, make more pods”.

We will create a HPA and then use OSToy to generate CPU intensive workloads. We will then observe how the HPA will scale up the number of pods in order to handle the increased workloads.

### 1. Create the Horizontal AutoScaler
 
 Run the following command to create the HPA. This will create an HPA that maintains between 1 and 10 replicas of the pods controlled by the ostoy-microservice deployment created. Roughly speaking, the HPA will increase and decrease the number of replicas (via the deployment) to maintain an average CPU utilization across all pods of 80% (since each pod requests 50 millicores, this means average CPU usage of 40 millicores).
 
 oc autoscale deployment/ostoy-microservice --cpu-percent=80 --min=1 --max=10
 
 
### 2. View the current pod number
 
 In the OSToy app in the left menu, click on “Autoscaling” to access this portion of the workshop.

![image](https://user-images.githubusercontent.com/32516987/216839226-43f1a170-ba91-4a8a-853f-bcadd2c8b71e.png)

 As was in the networking section you will see the total number of pods available for the microservice by counting the number of colored boxes. In this case we have only one. This can be verified through the web console or from the CLI.
 
 ![image](https://user-images.githubusercontent.com/32516987/216839240-b75f22b8-401b-4ff0-b8d3-656e9282571a.png)

 You can use the following command to see the running microservice pods only:

 > oc get pods --field-selector=status.phase=Running | grep microservice
 
 ### 3. Increase the Load

Since we now only have one pod, let’s increase the workload that the pod needs to perform. Click the link in the center of the card that says “increase the load”. ** Please click only ONCE! **
 
 ### 4. See the pods scale up
 
 > oc get pods --field-selector=status.phase=Running

### 5. Review Resources in included observability
 
 In the OpenShift web console left menu, click on Observe > Dashboards

In the dashboard, select Kubernetes / Compute Resources / Namespace (Pods) and our namespace ostoy.
 
![image](https://user-images.githubusercontent.com/32516987/216839334-7de0293e-803f-4825-9170-59cf29cfc373.png)

 
